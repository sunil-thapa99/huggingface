{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959ab725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_connection import *\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71837a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The weather in New York is sunny', tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")\n",
    "tool.call(\"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74087295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(content=\"The provided context does not contain any information regarding research on the impact of AI on the future of work and society. The files mentioned discuss communities promoting empathy and an environmental conservationist focused on wetland ecosystems. There is no data available to answer queries about AI's influence on work and society based solely on the given context.\", tool_name='some useful name', raw_input={'input': 'Responds about research on the impact of AI on the future of work and society?'}, raw_output=Response(response=\"The provided context does not contain any information regarding research on the impact of AI on the future of work and society. The files mentioned discuss communities promoting empathy and an environmental conservationist focused on wetland ecosystems. There is no data available to answer queries about AI's influence on work and society based solely on the given context.\", source_nodes=[NodeWithScore(node=TextNode(id_='0aed443a-b9ae-4b03-8c5a-e97012e068ec', embedding=None, metadata={'file_path': '/Users/sunilthapa/Desktop/projects/Huggingface/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-07-23', 'last_modified_date': '2025-07-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8f5d1b88-c3bd-451d-aa8f-113c543c3101', node_type='4', metadata={'file_path': '/Users/sunilthapa/Desktop/projects/Huggingface/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-07-23', 'last_modified_date': '2025-07-23'}, hash='c8050445025de1548b9071378be6494c191700224ea763f7f4f09c9eb0869b10'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='75338904-d807-45f3-80c9-917b2532a56b', node_type='1', metadata={'file_path': '/Users/sunilthapa/Desktop/projects/Huggingface/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-07-23', 'last_modified_date': '2025-07-23'}, hash='1a51a492734fc61f8caa3e315923fc7d8fd1e505fb021c37f73a736498be5722'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a5a70263-6bb2-4e22-8844-9e49f6940989', node_type='1', metadata={}, hash='7b0d068c81fe9cfdf6c12a9a6ffa2b197e73c9f3f96f853c9a4dda90ace0baf3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='communities to promote empathy', mimetype='text/plain', start_char_idx=116, end_char_idx=146, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.38228874291536624), NodeWithScore(node=TextNode(id_='63b3409c-0cf9-4544-9044-4bea7f8daad1', embedding=None, metadata={'file_path': '/Users/sunilthapa/Desktop/projects/Huggingface/data/persona_100.txt', 'file_name': 'persona_100.txt', 'file_type': 'text/plain', 'file_size': 107, 'creation_date': '2025-07-23', 'last_modified_date': '2025-07-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1ec71ed5-96f4-46c3-88cf-9ebd0dd4f9f7', node_type='4', metadata={'file_path': '/Users/sunilthapa/Desktop/projects/Huggingface/data/persona_100.txt', 'file_name': 'persona_100.txt', 'file_type': 'text/plain', 'file_size': 107, 'creation_date': '2025-07-23', 'last_modified_date': '2025-07-23'}, hash='99be4c588429bba859100e9a829e19f9d589ef3a819cb830396d9a9d4241d4c0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An environmental conservationist focused on wetland ecosystems and their role in mitigating climate change.', mimetype='text/plain', start_char_idx=0, end_char_idx=107, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.38069113316451214)], metadata={'0aed443a-b9ae-4b03-8c5a-e97012e068ec': {'file_path': '/Users/sunilthapa/Desktop/projects/Huggingface/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-07-23', 'last_modified_date': '2025-07-23'}, '63b3409c-0cf9-4544-9044-4bea7f8daad1': {'file_path': '/Users/sunilthapa/Desktop/projects/Huggingface/data/persona_100.txt', 'file_name': 'persona_100.txt', 'file_type': 'text/plain', 'file_size': 107, 'creation_date': '2025-07-23', 'last_modified_date': '2025-07-23'}}), is_error=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# llm = HuggingFaceInferenceAPI(model_name=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "llm = HuggingFaceInferenceAPI(model=\"http://127.0.0.1:1234/v1\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"some useful name\",\n",
    "    description=\"some useful description\",\n",
    ")\n",
    "await tool.acall(\n",
    "    \"Responds about research on the impact of AI on the future of work and society?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681747fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-tools-google -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a576ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x33f439510>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x33f439900>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x33f439b70>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x33f439c00>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x33f439db0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x33f439e10>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "tool_spec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59f65f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('load_data',\n",
       "  \"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\"),\n",
       " ('search_messages',\n",
       "  \"search_messages(query: str, max_results: Optional[int] = None)\\n\\n        Searches email messages given a query string and the maximum number\\n        of results requested by the user\\n           Returns: List of relevant message objects up to the maximum number of results.\\n\\n        Args:\\n            query (str): The user's query\\n            max_results (Optional[int]): The maximum number of search results\\n            to return.\\n\\n        \"),\n",
       " ('create_draft',\n",
       "  \"create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\\n\\n        Create and insert a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n\\n        \"),\n",
       " ('update_draft',\n",
       "  \"update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\\n\\n        Update a draft email.\\n           Print the returned draft's message and id.\\n           This function is required to be passed a draft_id that is obtained when creating messages\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n            draft_id (str): the id of the draft to be updated\\n\\n        \"),\n",
       " ('get_draft',\n",
       "  \"get_draft(draft_id: str = None) -> str\\n\\n        Get a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n\\n        \"),\n",
       " ('send_draft',\n",
       "  \"send_draft(draft_id: str = None) -> str\\n\\n        Sends a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n\\n        \")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30255578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "# We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server.\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "\n",
    "# get the agent\n",
    "agent = await get_agent(mcp_tool)\n",
    "\n",
    "# create the agent context\n",
    "agent_context = Context(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
